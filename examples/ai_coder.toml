[file]
name='ai_coder'
language='python'
description='An AI coding program'
[project]
requirements='''Write a program called AI coder, that taks a toml file as requirement to a program, then generate source code for the program
The toml file has two sections:
1. file section: contains the following fields:
    * name: the name of the program
    * language: the programming language of the source code
    * description: a brief description of the program
    * extension(optional): the file extension of the source code
        - if extension field is missing, common language<-->extension mapping such as python<-->py should be used, create a table cover at least python, bash, javascript.
2. project section: contains the following fields:
    * requirements: a detailed description of the program
    * specs(optional): specs for the program to use.   Specs are information that the program should reference to when generating the code
    * hints(optional): hints for the program, hints are used to guide the AI coder in generating the source code.

AI coder takes the toml file from command line argument, then generate the source code for the program based on the toml file.  The format of toml file is described as above.  Please use both the requirements and specs to generate the source code.  If hints are provided, use them to guide the AI coder in generating the source code.

After AI coder generates the source code, use reflection to review the generated code with a critic role.  The critic role act as supervisor of AI coder and try to find out bugs.  Critic should only focus on bugs in the code.

After critic provide feedback, a new LLM call would take the requirement, the source code, and the critic feedback, then generate a new source code based on the feedback.  The newly generated source code is the final version.

When generating the source code, AI coder should follow the instruction in specs.

An example of using ai_coder to generate helloworld.py is as follow:
python ai_coder.py helloworld.toml
Code saved to helloworld.py
'''
specs='''
## spec 1
In order to generate code, AI coder should utilize DeepSeek 'chat prefix completion' feature, the description of this feature is as follow:

    The chat prefix completion follows the Chat Completion API, where users provide an assistant's prefix message for the model to complete the rest of the message.

    Notice
    When using chat prefix completion, users must ensure that the role of the last message in the messages list is assistant and set the prefix parameter of the last message to True.
    The user needs to set base_url="https://api.deepseek.com/beta" to enable the Beta feature.
    Sample Code
    Below is a complete Python code example for chat prefix completion. In this example, we set the prefix message of the assistant to "`"+"``python\n" to force the model to output Python code, and set the stop parameter to ['`'+'``'] to prevent additional explanations from the model.

    from openai import OpenAI

    client = OpenAI(
        api_key="<your api key>",
        base_url="https://api.deepseek.com/beta",
    )

    messages = [
        {"role": "user", "content": "Please write quick sort code"},
        {"role": "assistant", "content": "`"+"``python\n", "prefix": True}
    ]
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=messages,
        stop=["`"+"``\n"],
    )
    print(response.choices[0].message.content)

## spec 2
api_token is taken from OPENAI_API_KEY environment variable, deepseek base_url is taken from OPENAI_API_URL   env. model name is taken from OPENAI_API_MODEL env.
'''
hints='''
## hint 1
For any code you want to generate "```", use the following instead:
"`"+"``", the purpose is to avoid ``` in the code to stop code generation
## hint 2
When ask LLM to generate code, usually it will be helpful if add a few lines describe how good the LLM is in writing code.  Instead of ask it to generate code directly.   Use this method when you compose prompt for LLM.  Elaborate on how good the LLM is in writing code, i.e. how long the experience is in specific language, what grade it has, etc.  Use your imagination
## hint 3
Here is an example of critic prompt for LLM:
Hereâ€™s code intended for task X: [previously generated code]
Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.
## hint 4
don't forget to import 'sys'!  You always forget to import sys!
## hint 5
when reviewing code, don't use chat prefix completion, use normal chat completion instead
## hint 6
please always pass necessary argument to function call before using them inside function body!
## hint 7, when using chat prefix completion, note the language should be the same as the one in the toml file
'''
